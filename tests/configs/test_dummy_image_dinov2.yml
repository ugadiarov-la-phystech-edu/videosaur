experiment_group: test
experiment_name: test_dummy_image

globals:
  NUM_SLOTS: 7
  SLOT_DIM: 64
  DINO_MODEL: vit_large_patch14_dinov2.lvd142m
  FEAT_DIM: 1024
  NUM_PATCHES: 1369
  NUM_GPUS: 1
  BATCH_SIZE_PER_GPU: 2
  TOTAL_BATCH_SIZE: "${mul: ${.NUM_GPUS}, ${.BATCH_SIZE_PER_GPU}}"
  BASE_LR: 0.0001
  SIM_WEIGHT: 0.1
  SIM_TEMP: 0.15

dataset:
  name: DummyDataModule
  train_size: 20
  val_size: 2
  batch_size: ${globals.BATCH_SIZE_PER_GPU}
  train_transforms:
    name: dummyimage_train
    type: image
    input_size: 518
    num_classes: 4
  val_transforms:
    name: dummyimage_val
    type: image
    input_size: 518
    num_classes: 4
  shapes:
    image: [518, 518, 3]
    masks: [518, 518, 1]

trainer:
  max_steps: 100

optimizer:
  name: Adam
  lr: 3e-4

model:
  input_type: image
  visualize: false
  visualize_every_n_steps: 10000

  initializer:
    name: RandomInit
    n_slots: ${globals.NUM_SLOTS}
    dim: ${globals.SLOT_DIM}

  encoder:
    backbone:
      name: ViTExtractor
      config_path: videosaur/dinov2/configs/eval/vitl14_reg4_pretrain.yaml
      checkpoint_path: checkpoint/dinov2/vitl/training_374999/teacher_checkpoint.pth
      n_last_layers_features: 1
      attention_features: keys
      frozen: true
    output_transform:
      name: networks.two_layer_mlp
      inp_dim: ${globals.FEAT_DIM}
      outp_dim: ${globals.SLOT_DIM}
      hidden_dim: "${mul: ${globals.FEAT_DIM}, 2}"
      layer_norm: true
    main_features_key: vit_block24

  grouper:
    name: SlotAttention
    inp_dim: ${globals.SLOT_DIM}
    slot_dim: ${globals.SLOT_DIM}
    n_iters: 2
    use_mlp: false

  decoder:
    name: MLPDecoder
    inp_dim: ${globals.SLOT_DIM}
    outp_dim: ${globals.FEAT_DIM}
    hidden_dims: [2048, 2048, 2048]
    n_patches: ${globals.NUM_PATCHES}

val_metrics:
  ari:
    name: ImageARI
    ignore_background: true
    pred_key: decoder_masks_hard
    true_key: masks
